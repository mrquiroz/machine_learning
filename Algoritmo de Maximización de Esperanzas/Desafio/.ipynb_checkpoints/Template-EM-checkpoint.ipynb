{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desafío - Inferencia de tópicos con EM\n",
    "\n",
    "* Para realizar este desafío debes haber revisado la lectura y videos correspondiente a la unidad.\n",
    "* Crea una carpeta de trabajo y guarda todos los archivos correspondientes (notebook y csv).\n",
    "* Una vez terminado el desafío, comprime la carpeta y sube el .zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción\n",
    "* En esta sesión trabajaremos con una serie de base de datos sobre letras musicales de distintos artistas. Cada uno de los csv se encuentra en la carpeta dump.\n",
    "* Cada csv tiene el nombre del artista a analizar. Los archivos contienen el nombre del artista, el género musical del artista, el nombre de la canción y las letras.\n",
    "* En base a esta información, el objetivo del ejercicio es generar un modelo probabilístico que pueda identificar el género musical más probable dado la letra de una canción. Para ello implementaremos un modelo conocido como Latent Dirichlet Allocation que hace uso de una variante del algoritmo EM para inferir clases latentes a partir de una matriz de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1: Preparar el ambiente de trabajo\n",
    "* Importe los módulos numpy , pandas , matplotlib , seaborn , glob y os siguiendo las buenas prácticas. Los últimos dos módulos permitirán realizar la importación de múltiples archivos dentro de la carpeta dump .\n",
    "* Para ello genere un objeto que guarde en una lista todos los archivos alojados en dump utilizando glob.glob y os.getcwd() para extraer las rutas absolutas. Posteriormente genere un objeto pd.DataFrame que contenga todos los csv.\n",
    "* Asegúrese de eliminar la columna Unnamed: 0 que se genera por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rutas = glob.glob(os.getcwd()+'/dump/*.csv')\n",
    "datas = []\n",
    "for ruta in rutas:\n",
    "    df = pd.read_csv(ruta).drop(columns='Unnamed: 0')\n",
    "    datas.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9489 entries, 0 to 129\n",
      "Data columns (total 4 columns):\n",
      "0    9489 non-null object\n",
      "1    9489 non-null object\n",
      "2    9489 non-null object\n",
      "3    9489 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 370.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat(datas)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artista</th>\n",
       "      <th>Genero</th>\n",
       "      <th>Cancion</th>\n",
       "      <th>Letra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Public Enemy</td>\n",
       "      <td>hiphop</td>\n",
       "      <td>You're Gonna Get Yours</td>\n",
       "      <td>(Flavor Flav) \\n Oh-oh Chuck, they out to get ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Public Enemy</td>\n",
       "      <td>hiphop</td>\n",
       "      <td>Sophisticated Bitch</td>\n",
       "      <td>That woman in the corner, cold playin' the rol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Public Enemy</td>\n",
       "      <td>hiphop</td>\n",
       "      <td>Miuzi Weighs A Ton</td>\n",
       "      <td>Yo Chuck, run a power move on them \\n (Yeah) \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Public Enemy</td>\n",
       "      <td>hiphop</td>\n",
       "      <td>Timebomb</td>\n",
       "      <td>(Intro - Flavor Flav) \\n Hey, Chuck, we got so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Public Enemy</td>\n",
       "      <td>hiphop</td>\n",
       "      <td>Too Much Posse</td>\n",
       "      <td>(Intro - Flavor Flav) \\n What do you got to sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Artista  Genero                 Cancion  \\\n",
       "0  Public Enemy  hiphop  You're Gonna Get Yours   \n",
       "1  Public Enemy  hiphop     Sophisticated Bitch   \n",
       "2  Public Enemy  hiphop      Miuzi Weighs A Ton   \n",
       "3  Public Enemy  hiphop                Timebomb   \n",
       "4  Public Enemy  hiphop          Too Much Posse   \n",
       "\n",
       "                                               Letra  \n",
       "0  (Flavor Flav) \\n Oh-oh Chuck, they out to get ...  \n",
       "1  That woman in the corner, cold playin' the rol...  \n",
       "2  Yo Chuck, run a power move on them \\n (Yeah) \\...  \n",
       "3  (Intro - Flavor Flav) \\n Hey, Chuck, we got so...  \n",
       "4  (Intro - Flavor Flav) \\n What do you got to sa...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns=['Artista','Genero','Cancion','Letra']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rock      4140\n",
       "hiphop    2535\n",
       "metal     1582\n",
       "pop       1232\n",
       "Name: Genero, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Genero.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Matriz de ocurrencias\n",
    "* Importe la clase CountVectorizer dentro de los módulos feature_extraction.text de la librería sklearn .\n",
    "* Aplique la clase para extraer las 5000 palabras más repetidas en toda la base de datos.\n",
    "* Con la clase inicializada, incorpore las letras con el método fit_transform y guarde los resultados en un nuevo objeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3: Entrenamiento del Modelo\n",
    "* Importe sklearn.decomposition.LatentDirichletAllocation y sklearn.model_selection.GridSearchCV .\n",
    "* Genere una búsqueda de grilla con los siguientes hiperparámetros:\n",
    "    * n_components: [5, 10, 15] .\n",
    "    * learning_decay: [0.7, 0.5] .\n",
    "* Entrene la búsqueda de grilla con las letras en un formato vectorizado con CountVectorizer .\n",
    "* Reporte brevemente cuál es la mejor combinación de hiperparámetros\n",
    "\n",
    "##### Digresión: Latent Dirichlet Allocation\n",
    "* Latent Dirichlet Allocatio (LDA) es un modelo probabilístico generativo basado en Inferencia Variacional EM. La principal utilidad de éste es la identificación de tópicos en un corpus de texto. \n",
    "* El proceso de inferencia se puede resumir en los siguientes pasos:\n",
    "    * Cada documento dentro del corpus se puede entender como una mezcla de tópicos comunes a nivel de corpus.\n",
    "    * Esta mezcla de tópicos es latente $\\leadsto$ Sólo observamos los documentos registrados y sus palabras. La API de sklearn.decomposition.LatentDirichletAllocation presenta la misma funcionalidad de todo modelo de sklearn. Algunos puntos a considerar en la inicialización de la clase son:\n",
    "        * n_components : Cantidad de tópicos a inferir en un corpus.\n",
    "        * learning_method : Forma en la que entran los datos en entrenamiento. Cuando es 'batch' , se ingresa la matriz de entrenamiento completa. Cuando es 'online' , la matriz de entrenamiento ingresa de manera secuencial en parcelas pequeñas.\n",
    "        * learning_decay : Tasa de aprendizaje en la función de pérdida. Cuando se implementa con learning_method='online', el modelo se entrena con Gradiente Estocástico Descendente.\n",
    "        * Perplejidad: Busca aproximar el número óptimo de tópicos a inferir. Técnicamente evalúa qué tan bien predice una muestra específica. En funcion a un número de tópicos, define la distribución teórica de palabras representada por los tópicos y la compara con la ocurrencia empírica de palabras en tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(learning_method='online')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer=CountVectorizer(stop_words='english',max_features=5000)\n",
    "count_vectorizer_fit = count_vectorizer.fit_transform(df.Letra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df['Letra'],df['Genero'],test_size=.33,random_state=11238)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_candidates = [\n",
    "  {'n_components': [5, 10, 15], 'learning_decay': [0.7, 0.5]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(lda, parameter_candidates, cv=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 14s, sys: 2.04 s, total: 2min 16s\n",
      "Wall time: 5min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise-deprecating',\n",
       "             estimator=LatentDirichletAllocation(batch_size=128,\n",
       "                                                 doc_topic_prior=None,\n",
       "                                                 evaluate_every=-1,\n",
       "                                                 learning_decay=0.7,\n",
       "                                                 learning_method='online',\n",
       "                                                 learning_offset=10.0,\n",
       "                                                 max_doc_update_iter=100,\n",
       "                                                 max_iter=10,\n",
       "                                                 mean_change_tol=0.001,\n",
       "                                                 n_components=10, n_jobs=None,\n",
       "                                                 perp_tol=0.1,\n",
       "                                                 random_state=None,\n",
       "                                                 topic_word_prior=None,\n",
       "                                                 total_samples=1000000.0,\n",
       "                                                 verbose=0),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid=[{'learning_decay': [0.7, 0.5],\n",
       "                          'n_components': [5, 10, 15]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid.fit(count_vectorizer_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_decay': 0.7, 'n_components': 5}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 4 : Inferencia e Identificación de Tópicos\n",
    "* En base a la mejor combinación de hiperparámetros, entrene el modelo con la matriz de atributos de las letras.\n",
    "* Para identificar de qué se trata cada tópico, necesitamos identificar las principales 15 palabras asociadas con éste. Puede implementar la siguiente línea de código para identificar las principales palabras en un tópico:\n",
    "\n",
    "```python\n",
    "# mediante .components_ podemos extraer una matriz que entrega las distribución de palabras por cada tópico.\n",
    "\n",
    "for topic_id, topic_name in enumerate(fit_best_lda.components_):\n",
    "    # para cada tópico\n",
    "    print(\"tópico: {}\".format(topic_id + 1))\n",
    "    # mediante argsort logramos ordenar los elementos por magnitud\n",
    "    # para los elementos más relevantes ordenados por argsort, buscamos su correlativo\n",
    "    # en la matriz dispersa y devolvemos el nombre.\n",
    "    # finalmente concatenamos las palabras\n",
    "    print(\" \".join([counter.get_feature_names()[i] for i in topic_name.argsort()[:-15 - 1: -1]]))\n",
    "```\n",
    "\n",
    "* Comente a qué tópicos está asociada cada clase inferida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tópico: 1\n",
      "la night man light black come day sun like old hand eyes home sky new\n",
      "tópico: 2\n",
      "oh yeah baby love got hey come girl wanna ooh know ah let gonna don\n",
      "tópico: 3\n",
      "like got shit don ain yo know man cause fuck just em nigga ya yeah\n",
      "tópico: 4\n",
      "god death dead die blood life kill pain soul power hell head mind end world\n",
      "tópico: 5\n",
      "don just know ll love ve time like say way feel away want let make\n"
     ]
    }
   ],
   "source": [
    "for topic_id, topic_name in enumerate(grid.best_estimator_.components_):\n",
    "    # para cada tópico\n",
    "    print(\"tópico: {}\".format(topic_id + 1))\n",
    "    # mediante argsort logramos ordenar los elementos por magnitud\n",
    "    # para los elementos más relevantes ordenados por argsort, buscamos su correlativo\n",
    "    # en la matriz dispersa y devolvemos el nombre.\n",
    "    # finalmente concatenamos las palabras\n",
    "    print(\" \".join([count_vectorizer.get_feature_names()[i] for i in topic_name.argsort()[:-15 - 1: -1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 5: Identificación de probabilidades\n",
    "* En base a la información generada, es posible identificar cuales van a ser los géneros más probables de ocurrir para un artista.\n",
    "* Para ello necesitamos guardar la probabilidad de cada canción en nuestra base de datos original. \n",
    "* Podemos implementar esto de la siguiente manera:\n",
    "    \n",
    "```python    \n",
    "# generamos una transformación de los datos a distribución de tópico por palabra en el documento\n",
    "fit_best_lda = best_lda.transform(transformed_feats)\n",
    "# estra transformación la podemos coercionar a un dataframe de la siguiente manera\n",
    "topics_for_each_doc = pd.DataFrame(\n",
    "                                    # pasamos esta matriz y la redondeamos en 3 decimales\n",
    "                                    np.round(fit_best_lda, 3),\n",
    "                                    # agregamos un índice\n",
    "                                    index=df_lyrics.index\n",
    "                                    )\n",
    "#agregamos identificadores de columna\n",
    "topics_for_each_doc.columns = list(map(lambda x: \"T: {}\".format(x), range(1, best_lda.n_components + 1)))\n",
    "# concatenamos las probabilidades de tópico por documento a nuestra matriz original\n",
    "concatenated_df = pd.concat([df_lyrics, topics_for_each_doc], axis=1)\n",
    "# argmax en la matriz de tópicos\n",
    "concatenated_df['highest_topic'] = np.argmax(docs_topics.values, axis=1) + 1   \n",
    "\n",
    "```\n",
    "\n",
    "* Genere una matriz de correlaciones entre la probabilidad de tópicos inferidos. Comente brevemente cuales son las principales asociaciones existentes.\n",
    "\n",
    "* Con esta nueva base de datos, identifique las probabilidades de pertenencia para un artista específico.\n",
    "\n",
    "* Grafique la distribución de las probabilidades para algún artista en específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_best_lda = grid.best_estimator_.transform(count_vectorizer_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.06484370e-01, 8.05798407e-04, 7.03641268e-01, 7.99292486e-04,\n",
       "        1.88269271e-01],\n",
       "       [6.48326121e-02, 2.53365109e-02, 5.73442769e-01, 9.58289891e-04,\n",
       "        3.35429818e-01],\n",
       "       [8.51653483e-04, 2.83075290e-02, 6.09733401e-01, 2.84339464e-01,\n",
       "        7.67679525e-02],\n",
       "       ...,\n",
       "       [8.11358764e-04, 8.15228824e-04, 7.42076681e-01, 8.03110160e-04,\n",
       "        2.55493621e-01],\n",
       "       [6.95764018e-02, 7.06569120e-02, 8.40748722e-01, 1.82942348e-02,\n",
       "        7.23729560e-04],\n",
       "       [1.63930685e-01, 1.52806404e-02, 8.04795805e-01, 1.51511166e-02,\n",
       "        8.41752164e-04]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_best_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_for_each_doc = pd.DataFrame(\n",
    "                                    # pasamos esta matriz y la redondeamos en 3 decimales\n",
    "                                    np.round(fit_best_lda, 3),\n",
    "                                    # agregamos un índice\n",
    "                                    index=df.index\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.106</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.065</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.088</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4\n",
       "0  0.106  0.001  0.704  0.001  0.188\n",
       "1  0.065  0.025  0.573  0.001  0.335\n",
       "2  0.001  0.028  0.610  0.284  0.077\n",
       "3  0.001  0.181  0.670  0.092  0.056\n",
       "4  0.088  0.002  0.694  0.002  0.213"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_for_each_doc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_for_each_doc.columns = list(map(lambda x: \"T: {}\".format(x), range(1, grid.best_estimator_.n_components + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T: 1</th>\n",
       "      <th>T: 2</th>\n",
       "      <th>T: 3</th>\n",
       "      <th>T: 4</th>\n",
       "      <th>T: 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.106</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.065</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.088</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    T: 1   T: 2   T: 3   T: 4   T: 5\n",
       "0  0.106  0.001  0.704  0.001  0.188\n",
       "1  0.065  0.025  0.573  0.001  0.335\n",
       "2  0.001  0.028  0.610  0.284  0.077\n",
       "3  0.001  0.181  0.670  0.092  0.056\n",
       "4  0.088  0.002  0.694  0.002  0.213"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_for_each_doc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df = pd.concat([df, topics_for_each_doc], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-24528f72ab94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconcatenated_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'highest_topic'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenero\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \"\"\"\n\u001b[0;32m-> 1103\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "concatenated_df['highest_topic'] = np.argmax(df.Genero.values, axis=1) + 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
