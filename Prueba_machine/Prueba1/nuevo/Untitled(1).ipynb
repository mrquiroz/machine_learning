{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El objetivo general de esta prueba es alcanzar el mejor desempeño posible para clasificar si un tweet es positivo o negativo.\n",
    "## Para medir el desempeño, se evaluará con un conjunto de datos del cuál no tendrán acceso. De esta manera evitaremos que los modelos aprendan información sobre el conjunto de validación.\n",
    "### Crea una carpeta de trabajo y guarda todos los archivos correspondientes (notebook, archivos auxiliares y csv). Una vez terminada la prueba, comprime la carpeta y sube el .zip a la sección correspondiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/moisesquiroz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/moisesquiroz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stap = stopwords.words(\"english\")\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st = LancasterStemmer()\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import re\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.-Generar un análisis exploratorio sobre los datos contenidos en el DataFrame, considerando palabras más comunes y distribución de las clases en el vector objetivo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LO QUE HAREMOS, SERÁ CREAR MODELOS DE REGRESIÓN LOGÍSTICA, BAYES, ARBOLES DE DECISIÓN, RANDOMFORESTCLASS, SVM, LDA, Adaboost Y Gradiente BUSCANDO LOS MEJORES HIPERPARÁMETROS CON GRIDSEARCH. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ingrese el nombre del archivo:training_tweets.csv\n"
     ]
    }
   ],
   "source": [
    "nombre_archivo = input(\"ingrese el nombre del archivo:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(nombre_archivo).drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Preparación del vector objetivo y las matrices de entrenamiento y validación:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].map({'love':1,'worry':-1,'happiness':1,'fun':1,'hate':-1,'sadness':-1,'empty':-1,'relief':1,'surprise':1,'enthusiasm':1,'boredom':-1,'anger':-1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cambiar_de_a_poco(Dataframe, col):\n",
    "    from nltk.corpus import stopwords\n",
    "    stap = stopwords.words(\"english\")\n",
    "    Dataframe[col] = Dataframe[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stap)])) #Saca stopwords\n",
    "    Dataframe[col] = Dataframe[col].apply(lambda x: ' '. join([word.lower() for word in x.split()])) #minúsculas\n",
    "    Dataframe[col] = pd.Series(map(lambda x: re.sub(\"((www\\.[^\\s]+)|(https?://[^\\s]+))\", \"\", x), Dataframe[col])) #Convierte url's \n",
    "    Dataframe[col] = pd.Series(map(lambda x: re.sub(\"@[^\\s]+\", \"\", x), Dataframe[col])) #Elimina usuarios\n",
    "    Dataframe[col] = pd.Series(map(lambda x: re.sub(r\"#([^\\s]+)\", r\"\\1\", x), Dataframe[col])) #Saca los Hashtags\n",
    "    Dataframe[col] = pd.Series(map(lambda x: re.sub('[\\s]+', ' ', x), Dataframe[col]))  #Elimina dobles espacios\n",
    "    Dataframe[col] = pd.Series(map(lambda x: \"\".join([char for char in x if char not in string.punctuation]), Dataframe[col])) #Eliminar puntuación\n",
    "    Dataframe[col] = pd.Series(map(lambda x: \"\".join([char for char in x if char not in string.punctuation]), Dataframe[col])) #Eliminar puntuación\n",
    "    Dataframe[col] = Dataframe[col].apply(lambda x: ' '. join([lmtzr.lemmatize(word) for word in x.split()])) #Generaliza palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cambiar_de_a_poco(df, \"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop(index=df[df.sentiment.isna()].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### se borraron los index donde son null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df2[\"sentiment\"], kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=stap, lowercase=True)\n",
    "count_vectorizer_fit = count_vectorizer.fit_transform(df2[\"content\"])\n",
    "words = count_vectorizer.get_feature_names()\n",
    "words_freq = count_vectorizer_fit.toarray().sum(axis=0)\n",
    "aux = pd.DataFrame([words, words_freq]).T\n",
    "aux.columns = [\"Palabra\", \"Freq\"]\n",
    "aux = aux.sort_values(by=['Freq'],ascending=False)\n",
    "plt.figure(figsize=(15,25))\n",
    "plt.barh(y=aux[\"Palabra\"][:100], width=aux[\"Freq\"][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counterizar(DataFrame, colname):\n",
    "    countvec = CountVectorizer(stop_words=stap, lowercase=True)\n",
    "    countvec_fit = countvec.fit_transform(DataFrame[colname])\n",
    "    return countvec_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Las palabras más utilizadas que puedes servir para discriminar el sentimiento que hay detrás de las palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Entrenamiento de modelos: \n",
    "    En base a los modelos vistos en clase, implemente por lo menos 5. Para cada uno de ellos justifique la elección de hiperparámetros. Si implementa búsqueda de grilla para cada uno de ellos, defina el rango de valores a tomar en cada hiperparámetro. \n",
    "    Reporte el desempeño de cada modelo en las muestras de entrenamiento y validación. Comente sobre la capacidad de generalización de cada uno de ellos haciendo uso de los conceptos vistos en el curso. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglog = LogisticRegression(n_jobs=-1, random_state=2503)\n",
    "naiveBayes = BernoulliNB(fit_prior=False)\n",
    "TreeClass = DecisionTreeClassifier(random_state=2503)\n",
    "SVCClass = SVC(random_state=2503)\n",
    "RandomClass = RandomForestClassifier(random_state=2503, n_jobs=-1)\n",
    "modelo_adaboost = AdaBoostClassifier(random_state=253)\n",
    "modelo_gradiente = GradientBoostingClassifier(random_state=253)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsreglog = {\"C\":[0.1, 0.5, 0.75, 1, 1.25, 1.5, 2], \"fit_intercept\": [True, False], \"max_iter\": [50, 100, 250, 500, 1000]}\n",
    "paramsnaive = {\"alpha\": [0.5, 0.7, 1]}\n",
    "paramsTree = {\"max_depth\": list(np.linspace(1,32,32)), \"max_features\": list(np.linspace(1,9,9, dtype=int))}\n",
    "paramssvc = {\"C\":[0.001, 0.01, 0.1, 1, 10, 200, 800], \"gamma\": [0.0001, 0.001, 0.01, 0.1, 5]}\n",
    "paramsrandomf = {\"max_depth\": [50, 65, 75, 90, 110], \"n_estimators\": np.linspace(20, 1000, 13, dtype=int)}\n",
    "params_adaboost= {'learning_rate': [0.01, 0.1,0.3, 0.5, 0.8],'n_estimators': [50, 100, 250, 500, 1000, 2000]}\n",
    "params_gradient = {'learning_rate': [0.01, 0.1,0.3,0.5, 0.8],'n_estimators': [50, 100, 250, 500, 1000, 2000],'subsample': [0.1,0.5,0.7, 0.9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gridreglog = GridSearchCV(reglog, param_grid=paramsreglog, cv=10, n_jobs = -1)\n",
    "gridbayes = GridSearchCV(naiveBayes, param_grid=paramsnaive, cv=10, n_jobs=-1)\n",
    "gridtree = GridSearchCV(TreeClass, param_grid=paramsTree, cv=10, n_jobs=-1)\n",
    "gridsvc = GridSearchCV(SVCClass, param_grid=paramssvc, cv=5, n_jobs=-1)\n",
    "gridrandomf = GridSearchCV(RandomClass, param_grid=paramsrandomf, cv=3, n_jobs=-1)\n",
    "gridada = GridSearchCV(modelo_adaboost, params_adaboost, cv=10, n_jobs=-1)\n",
    "gridgradiente = GridSearchCV(modelo_gradiente, params_gradient, cv=10, n_jobs=-1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(count_vectorizer_fit, df2[\"sentiment\"], test_size=.33, random_state = 2503)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gridreglog.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gridbayes.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gridtree.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "gridsvc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gridrandomf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gridada.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gridgradiente.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridreglog.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridbayes.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gabriel:\n",
    "    Para evaluar el desempeño de los modelos haz lo siguiente:\n",
    "    Ejecuta las importaciones del Notebook y las funciones cambiar_de_a_poco y counterizar\n",
    "    Lee tu dataframe\n",
    "    Aplica la función cambiar_de_a_poco\n",
    "    Importa los dos modelos\n",
    "    Evalúa los modelos dándoles como conjunto de X's , la función counterizar.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsvc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridtree.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridada.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridrandomf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridgradiente.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_gradiente2 = GradientBoostingClassifier(random_state=2503)\n",
    "params_gradient2 = {'learning_rate': [0.045, 0.075, 0.09, 0.1, 0.15, 0.18, 0.21],'n_estimators': [1500, 2000, 2500, 3000],'subsample': [0.7, 0.9, 0.95]}\n",
    "gridgradiente2 = GridSearchCV(modelo_gradiente, params_gradient2, cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gridgradiente2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridgradiente2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ada = gridada.best_estimator_\n",
    "model_bayes = gridbayes.best_estimator_\n",
    "model_gradiente = gridgradiente.best_estimator_\n",
    "model_gradiente2 = gridgradiente2.best_estimator_\n",
    "model_randomf = gridrandomf.best_estimator_\n",
    "model_reglog = gridreglog.best_estimator_\n",
    "model_svc = gridsvc.best_estimator_\n",
    "model_tree = gridtree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ada = model_ada.predict(x_test)\n",
    "y_pred_bayes = model_bayes.predict(x_test)\n",
    "y_pred_grad = model_gradiente.predict(x_test)\n",
    "y_pred_grad2 = model_gradiente2.predict(x_test)\n",
    "y_pred_randomf = model_randomf.predict(x_test)\n",
    "y_pred_reglog = model_reglog.predict(x_test)\n",
    "y_pred_svc = model_svc.predict(x_test)\n",
    "y_pred_tree = model_tree.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Modelo AdaBoost\")\n",
    "print(classification_report(y_pred_ada, y_test, digits=4))\n",
    "print(\"=\"*80)\n",
    "print(\"Modelo Bayes\")\n",
    "print(classification_report(y_pred_bayes, y_test, digits=4))\n",
    "print(\"=\"*80)\n",
    "print(\"Modelo Gradiente\")\n",
    "print(classification_report(y_pred_grad, y_test, digits=4))\n",
    "print(\"=\"*80)\n",
    "print(\"Modelo Gradiente.v2\")\n",
    "print(classification_report(y_pred_grad2, y_test, digits=4))\n",
    "print(\"=\"*80)\n",
    "print(\"Modelo Random Forest\")\n",
    "print(classification_report(y_pred_randomf, y_test, digits=4))\n",
    "print(\"=\"*80)\n",
    "print(\"Modelo Regresión Logística\")\n",
    "print(classification_report(y_pred_reglog, y_test, digits=4))\n",
    "print(\"=\"*80)\n",
    "print(\"Modelo SVC\")\n",
    "print(classification_report(y_pred_svc, y_test, digits=4))\n",
    "print(\"=\"*80)\n",
    "print(\"Modelo Árbol Class\")\n",
    "print(classification_report(y_pred_tree, y_test, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seleccione los 2 mejores modelos, serialicelos y envíelos a evaluación. \n",
    "    Recuerde que el modelo serializado debe ser posterior al fit , para poder ejecutar predict en los nuevos datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Los mejores dos modelos son: Regresión Logística y Bayes, ya que presentan mejor promedio de f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes_model = 'Bayes_MJpR.sav'\n",
    "pickle.dump(model_bayes, open(Bayes_model, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reg_log_model = 'Reglog_MJpR.sav'\n",
    "pickle.dump(model_reglog, open(Reg_log_model, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moisesquiroz/anaconda3/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator BernoulliNB from version 0.21.2 when using version 0.21.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/moisesquiroz/anaconda3/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.21.2 when using version 0.21.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#ABRIR LOS MODELOS DE LA SGTE MANERA:\n",
    "import pickle\n",
    "modelo_cargado = pickle.load(open(\"Bayes_MJpR.sav\", \"rb\"))\n",
    "\n",
    "modelo_cargado2 = pickle.load(open(\"Reglog_MJpR.sav\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input with 23788 features, got 27698 instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-64c92cd5fd6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodelo_cargado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounterizar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/moisesquiroz/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \"\"\"\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/moisesquiroz/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features_X\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m             raise ValueError(\"Expected input with %d features, got %d instead\"\n\u001b[0;32m--> 953\u001b[0;31m                              % (n_features, n_features_X))\n\u001b[0m\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0mneg_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_log_prob_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input with 23788 features, got 27698 instead"
     ]
    }
   ],
   "source": [
    "modelo_cargado.predict(counterizar(df2,\"content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>happy mama day mother</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i lost please help find good home</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yes yes i am networking whore fullestand girl ...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you wish would tweet followed me</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>microeconomics project ihate subject amp besid...</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  sentiment\n",
       "0                              happy mama day mother       love\n",
       "1                  i lost please help find good home      worry\n",
       "2  yes yes i am networking whore fullestand girl ...  happiness\n",
       "3                   you wish would tweet followed me    neutral\n",
       "4  microeconomics project ihate subject amp besid...      worry"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
